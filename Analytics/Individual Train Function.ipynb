{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.model_selection import cross_validate\n",
    "from tqdm import tnrange, tqdm_notebook, tqdm\n",
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import scipy.stats as stats\n",
    "import xgboost as xgb\n",
    "import requests as r\n",
    "import pandas as pd\n",
    "import seaborn as s\n",
    "import numpy as np\n",
    "import googlemaps\n",
    "import postgres\n",
    "import holidays\n",
    "import config\n",
    "import json\n",
    "import math\n",
    "import gc\n",
    "\n",
    "# enable automatic garbage collection\n",
    "gc.enable()\n",
    "\n",
    "ie_holidays = holidays.Ireland()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_distances = pd.read_csv(\"stored_queries/distancedata.csv\", header=None)\n",
    "stop_distances.columns = ['stopid','previous_stopid','distance']\n",
    "stop_distances.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = pd.read_csv(\"stop_information.csv\")\n",
    "cols = list(stops.columns)\n",
    "cols[0] = 'ix'\n",
    "stops.columns = cols\n",
    "stops.drop(columns=cols[0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weather Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.read_csv(\"stored_queries/weather.csv\")\n",
    "weather.icon = weather.icon.astype('category')\n",
    "weather.weatherdayofservice = pd.to_datetime(weather.weatherdayofservice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leavetimes Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"stored_queries/combined.csv\")\n",
    "data.columns = ['dayofservice','tripid','lineid','direction','progrnumber','stopid','plannedDEP','plannedARR','actualDEP','actualARR','routeid']\n",
    "gc.collect()\n",
    "\n",
    "data.drop(columns=['routeid','plannedDEP','plannedARR','actualDEP'], inplace=True)\n",
    "data.drop_duplicates()\n",
    "gc.collect()\n",
    "\n",
    "# data.dayofservice = pd.to_datetime(data.dayofservice.loc[:])\n",
    "gc.collect()\n",
    "\n",
    "data.sort_values(by=['dayofservice','lineid','tripid','direction','progrnumber'], inplace=True)\n",
    "gc.collect()\n",
    "\n",
    "data.drop(columns=['lineid'], inplace=True)\n",
    "\n",
    "# data.to_csv(\"stored_queries/combinedsorted.csv\", index=False, chunksize=500000)\n",
    "\n",
    "# del data\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "data.to_csv(\"stored_queries/combinedsorted.csv\", index=False, chunksize=500000)\n",
    "\n",
    "del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"stored_queries/combinedsorted.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "data.dayofservice = pd.to_datetime(data.dayofservice.loc[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove Stops which are no longer in service**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# ==================== REMOVE INACTIVE STOPS ====================== #\n",
    "active_stopids = stops.stopid.values\n",
    "\n",
    "# remove all inactive stops from the dataset. -> additional models that arent needed. \n",
    "data = data[data.stopid.isin(active_stopids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make Arrays for previous stops etc..**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# previous stopid\n",
    "previousstops =  list(data.stopid)\n",
    "previousstops = np.array(previousstops[:-1]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# progrnumber of previous stopid\n",
    "previousstops_progrnumber = list(data.progrnumber)\n",
    "previousstops_progrnumber = np.array(previousstops_progrnumber[:-1]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Actual arrival time of previous stopid\n",
    "previousstops_actualARR = list(data.actualARR)\n",
    "previousstops_actualARR = np.array(previousstops_actualARR[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Delete the first row of the dataframe to shift the progrnumbers by one. \n",
    "data = data.iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "data['previous_stopid'] = previousstops\n",
    "data['previous_stopARR'] = previousstops_actualARR\n",
    "data['previous_progrnumber'] = previousstops_progrnumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "data = data[data.progrnumber != 1]\n",
    "data.dropna(inplace=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# recast type of integer cols from float to int. \n",
    "data.previous_stopid = data.previous_stopid.astype(int)\n",
    "data.previous_progrnumber = data.previous_progrnumber.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# make progrnumber difference column and then drop anything thats not exactly 1, removes data which skips stops. \n",
    "data['progrnumber_difference'] = data.progrnumber - data.previous_progrnumber\n",
    "\n",
    "# remove non-consecutive stop pairs.\n",
    "data = data[data.progrnumber_difference==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "data = data[['dayofservice', 'tripid','stopid', 'previous_stopid', 'actualARR', 'previous_stopARR']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "data.to_csv(\"stored_queries/combinedfinal.csv\", index=False, chunksize=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import ready to go leavetimes table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"stored_queries/combinedfinal.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dayofservice = pd.to_datetime(data.dayofservice.loc[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating stop_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_pairs = data[['stopid','previous_stopid']].drop_duplicates()\n",
    "\n",
    "print(\"There are %d unique pairs of stops in stop distances\" % (stop_pairs.count()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop through stop_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_stops = stop_pairs.count()[0]\n",
    "print(\"There are %d models to train.\" % no_stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_season(x):\n",
    "    winter = [11,12,1]\n",
    "    autumn = [10,9,8]\n",
    "    spring = [4,3,2]\n",
    "\n",
    "    if x in winter:\n",
    "        return 'Winter'\n",
    "    elif x in autumn:\n",
    "        return 'Autumn'\n",
    "    elif x in spring:\n",
    "        return 'Spring'\n",
    "    else:\n",
    "        return 'Summer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target     = ['travel_time']\n",
    "\n",
    "predictors = ['temperature','humidity', 'windSpeed', 'rain', 'hour', 'holiday', 'weekend',\n",
    "              'month','season_Winter','season_Autumn','season_Summer','season_Spring',\n",
    "              'icon_clear-day', 'icon_clear-night', 'icon_cloudy', 'icon_fog',\n",
    "              'icon_partly-cloudy-day', 'icon_partly-cloudy-night', 'icon_rain','icon_wind']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "models = dict()\n",
    "emptys = []\n",
    "metric = dict()\n",
    "Error_routes = []\n",
    "\n",
    "\n",
    "# Model Training parameters\n",
    "param = {\n",
    "    'tree_method':'gpu_hist',\n",
    "    'eta': 0.15,\n",
    "    'max_depth': 6\n",
    "}\n",
    "\n",
    "num_rounds = 10000\n",
    "\n",
    "for pair in tqdm_notebook(stop_pairs.iterrows(), desc=\"Progress: \", total=no_stops):\n",
    "    \n",
    "    # Travelling From A -> B\n",
    "    A = pair[1][1]\n",
    "    B = pair[1][0]\n",
    "    \n",
    "    # ========================= Rows from A -> B =========================== #\n",
    "    sample = data[(data.stopid==B)&(data.previous_stopid==A)]\n",
    "        \n",
    "    # ========================= Adding Weather Data ======================== #    \n",
    "    sample.actualARR = sample.dayofservice + pd.to_timedelta(sample.actualARR, unit = 'seconds') # in nanoseconds\n",
    "    sample.previous_stopARR = sample.dayofservice + pd.to_timedelta(sample.previous_stopARR, unit = 'seconds') # in nanoseconds\n",
    "\n",
    "    # new columns for combining\n",
    "    sample['weather_merge_time'] = sample.actualARR.dt.round('H') #  .dt useful\n",
    "    sample.sort_values(by='weather_merge_time', inplace=True)\n",
    "\n",
    "    # weather data\n",
    "    weather.weatherdayofservice = weather.weatherdayofservice + pd.to_timedelta(weather.hour, unit='hour')\n",
    "    weather.sort_values(by='weatherdayofservice', inplace=True)\n",
    "    \n",
    "    # using merge_asof to save data. getting nearest weather. \n",
    "    combinedata = pd.merge_asof(sample,\n",
    "                                weather[['icon',\n",
    "                                         'temperature',\n",
    "                                         'humidity',\n",
    "                                         'windSpeed',\n",
    "                                         'rain',\n",
    "                                         'weatherdayofservice',\n",
    "                                         'hour']], \n",
    "                                left_on='weather_merge_time', \n",
    "                                right_on='weatherdayofservice', \n",
    "                                direction='nearest')\n",
    "    \n",
    "    combinedata.drop(columns=['weather_merge_time','weatherdayofservice'], inplace=True)\n",
    "    \n",
    "    # ======================== Adding Holiday ============================= #\n",
    "    combinedata['holiday'] = combinedata.dayofservice.apply(lambda x: x in ie_holidays)\n",
    "    \n",
    "    # ======================== Adding weekend ============================= #\n",
    "    combinedata['weekend'] = combinedata.dayofservice.dt.weekday.isin([5,6])\n",
    "    \n",
    "    # ======================== Adding month   ============================= #\n",
    "    combinedata['month'] = combinedata.dayofservice.dt.month\n",
    "\n",
    "    # ======================== Adding season  ============================= #\n",
    "    combinedata['season'] = combinedata.dayofservice.dt.month.apply(set_season)\n",
    "    \n",
    "    # ======================== Adding travel_time  ============================= #    \n",
    "    combinedata['travel_time'] = (pd.to_timedelta(combinedata.actualARR) - \n",
    "                                  pd.to_timedelta(combinedata.previous_stopARR)).dt.seconds\n",
    "    \n",
    "    # drop any values less than 5 seconds [assumed erroneous]\n",
    "    combinedata = combinedata[combinedata.travel_time > 5]\n",
    "    \n",
    "    # ============== Encoding Categorical Data : Season  =================== #\n",
    "    combinedata.season = combinedata.season.astype('category', categories=['Summer','Spring','Autumn','Winter'])\n",
    "\n",
    "    combinedata = pd.concat([combinedata, pd.get_dummies(combinedata.season, prefix='season')], axis=1)\n",
    "    combinedata.drop(columns=['season'], inplace=True)\n",
    "\n",
    "    # ============== Encoding Categorical Data : Icons  ==================== #\n",
    "    combinedata.icon = combinedata.icon.astype('category', categories=['partly-cloudy-day', 'partly-cloudy-night', 'clear-day', 'clear-night', 'rain', 'fog', 'cloudy', 'wind'])\n",
    "\n",
    "    combinedata = pd.concat([combinedata, pd.get_dummies(combinedata.icon, prefix='icon')], axis=1)\n",
    "    combinedata.drop(columns=['icon'], inplace=True)\n",
    "\n",
    "    combinedata.dropna(inplace=True)\n",
    "    \n",
    "    modeldata = combinedata[['travel_time','temperature','humidity', 'windSpeed', 'rain', 'hour', \n",
    "                             'holiday', 'weekend','month','season_Winter','season_Autumn','season_Summer','season_Spring',\n",
    "                             'icon_clear-day', 'icon_clear-night', 'icon_cloudy', 'icon_fog',\n",
    "                             'icon_partly-cloudy-day', 'icon_partly-cloudy-night', 'icon_rain','icon_wind']]\n",
    "    \n",
    "    \n",
    "    # ============================== TRAINING ============================== #\n",
    "    \n",
    "    \n",
    "    if modeldata.count()[0] > 10:\n",
    "\n",
    "        # ========================= Removing Outliers ========================== #\n",
    "        travel_sigma = modeldata.travel_time.std()\n",
    "\n",
    "        # Only allow travel times greater than zero \n",
    "        modeldata = modeldata[modeldata.travel_time >= 0]\n",
    "\n",
    "        # Filter outliers from the dataset [ \\mu - 3σ < x < 3.5σ + μ ] - skewed dataset\n",
    "        modeldata = modeldata[modeldata.travel_time < modeldata.travel_time.mean() + 3.5*travel_sigma]\n",
    "        modeldata = modeldata[modeldata.travel_time > modeldata.travel_time.mean() - 2*travel_sigma]\n",
    "\n",
    "        # ========================= Remove Null Data =========================== #\n",
    "        modeldata.dropna(inplace=True)\n",
    "\n",
    "        # ========================= Test/Train Splits ========================== #\n",
    "        X_train, X_test, y_train, y_test = train_test_split(modeldata[predictors],modeldata[target].values.ravel(), test_size=0.25, shuffle=True)\n",
    "\n",
    "        # ========================== Making DMatrices ========================== #\n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "        # =========================== Training Model =========================== #\n",
    "\n",
    "        try:\n",
    "\n",
    "            # ============ Train ============= #\n",
    "            model = xgb.train(param, dtrain, num_rounds, evals=[(dtest, 'Test')], verbose_eval=False, early_stopping_rounds=100)\n",
    "            model.save_model(f\"ModelFiles/StopModels/{A}_{B}.model\")\n",
    "\n",
    "            # ============ Testing Accuracy ========== #\n",
    "            preds = model.predict(dtest)\n",
    "            metric[f'{A}_{B}'] = dict()\n",
    "            metric[f'{A}_{B}']['rmse'] = np.sqrt(metrics.mean_squared_error(preds, y_test))\n",
    "            metric[f'{A}_{B}']['preds']= preds\n",
    "            metric[f'{A}_{B}']['ytest']= y_test\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error with route: {A} -> {B}\")\n",
    "            Error_routes.append((A,B))\n",
    "\n",
    "    else:\n",
    "        print(f\"Empty Set Error: {A} -> {B} | Count: {modeldata.count()[0]} | Pre-Count: {sample.count()[0]}\")\n",
    "        emptys.append((A,B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run this to see how bad its fucked up m8\n",
    "print(len(emptys))\n",
    "\n",
    "rmses_array = []\n",
    "\n",
    "for row in metric:\n",
    "    rmses_array.append((metric[row]['rmse']))\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(metric[row]['preds'])\n",
    "    plt.plot(metric[row]['ytest'], alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(metric['226_228']['preds'])\n",
    "plt.plot(metric['226_228']['ytest'] - metric['226_228']['preds'],alpha=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import json \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import requests as req\n",
    "import holidays as hol\n",
    "ie_holidays = hol.Ireland()\n",
    "\n",
    "import importlib\n",
    "importlib.reload(config)\n",
    "\n",
    "from datetime import date, time, datetime\n",
    "\n",
    "def set_season(x):\n",
    "    winter = [11,12,1]\n",
    "    autumn = [10,9,8]\n",
    "    spring = [4,3,2]\n",
    "\n",
    "    if x in winter:\n",
    "        return 'Winter'\n",
    "    elif x in autumn:\n",
    "        return 'Autumn'\n",
    "    elif x in spring:\n",
    "        return 'Spring'\n",
    "    else:\n",
    "        return 'Summer'\n",
    "       \n",
    "\n",
    "def prediction_route(PDate, StopA, StopB, PTime):\n",
    "    \"\"\"\n",
    "    Return an estimate of travel time, in seconds, for a given journey. \n",
    "    \n",
    "    inputs:\n",
    "    ---------------------------------------\n",
    "    (str) PDate:            YYYY-MM-DD\n",
    "    (str) PTime:            HH:MM\n",
    "    (str) StopA:            Start Stop\n",
    "    (str) StopB:            End Stop\n",
    "    \n",
    "    \n",
    "    Outputs:\n",
    "    ---------------------------------------\n",
    "    (int) Travel Time:          Seconds\n",
    "    \"\"\"\n",
    "    \n",
    "    # =========================== Import Model ========================= #\n",
    "    \n",
    "    print('here')\n",
    "    \n",
    "    model = xgb.Booster()\n",
    "    model.load_model(f\"ModelFiles/StopModels/{StopA}_{StopB}.model\")\n",
    "    \n",
    "    print('here')\n",
    "    \n",
    "    # ====================== Dateand Time objects ====================== #\n",
    "    \n",
    "    ddate = date(int(PDate[:4]), int(PDate[5:7]), int(PDate[-2:]))\n",
    "    dtime = time(int(PTime[:2]), int(PTime[-2:]))\n",
    "        \n",
    "    # ========================== Weather Data ========================== # \n",
    "    # will need to sync this with the automated live weather updates to not waste calls. Also add forecasting option. \n",
    "    \n",
    "    weathercall = req.get(f\"https://api.darksky.net/forecast/{config.darksky_api}/53.3498,-6.2603\").content\n",
    "    weather = json.loads(weathercall)\n",
    "    weather= weather['currently']\n",
    "    \n",
    "    print('here')\n",
    "    \n",
    "    # ======================== Inputs DataFrame ======================== #\n",
    "    \n",
    "    predictors = ['temperature','humidity', 'windSpeed', 'rain', 'hour', 'holiday', 'weekend',\n",
    "                  'month','season_Winter','season_Autumn','season_Summer','season_Spring',\n",
    "                  'icon_clear-day', 'icon_clear-night', 'icon_cloudy', 'icon_fog',\n",
    "                  'icon_partly-cloudy-day', 'icon_partly-cloudy-night', 'icon_rain','icon_wind']  \n",
    "    \n",
    "    # Make dataframe of inputs. \n",
    "    inputs = pd.DataFrame(np.zeros(len(predictors))).T\n",
    "    inputs.columns = predictors\n",
    "    \n",
    "    inputs.hour = dtime.hour\n",
    "    inputs.month= ddate.month  \n",
    "    # ========================= Weather Columns ======================== #\n",
    "    \n",
    "    inputs.temperature = weather['temperature']\n",
    "    inputs.humidity = weather['humidity']\n",
    "    inputs.windSpeed = weather['windSpeed']\n",
    "    \n",
    "    # convert in inches of liquid water per hour to mm\n",
    "    inputs.rain = float(weather['precipIntensity'])/0.0394\n",
    "    \n",
    "    # ========================= Weekday/Weekend ======================== #    \n",
    "    \n",
    "    if ddate.weekday() in [5,6]:\n",
    "        inputs.weekday=False\n",
    "    else:\n",
    "        inputs.weekday=True \n",
    "    \n",
    "    # ===================== One Hot Encoded Columns ==================== #  \n",
    "    \n",
    "    inputs[\"icon_{0}\".format(weather['icon'])]=1\n",
    "    inputs[\"season_{0}\".format(set_season(ddate.month))]=1\n",
    "    \n",
    "    # ========================= Applying Model ========================= #\n",
    "    \n",
    "    print('here')\n",
    "    \n",
    "    inputdata = xgb.DMatrix(inputs)\n",
    "    estimate = model.predict(inputdata)\n",
    "    \n",
    "    # ======================= Adjust for #stops ======================== #\n",
    "    \n",
    "    estimate = estimate.tolist()[0]\n",
    "    \n",
    "    # ========================= Returning Data ========================= #\n",
    "    \n",
    "    return estimate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "here\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Return an estimate of travel time, in seconds, for a given journey. \n",
    "\n",
    "inputs:\n",
    "---------------------------------------\n",
    "(str) PDate:            YYYY-MM-DD\n",
    "(str) PTime:            HH:MM\n",
    "(str) StopA:            Start Stop\n",
    "(str) StopB:            End Stop\n",
    "\"\"\"\n",
    "\n",
    "PDate = \"2019-08-15\"\n",
    "PTime = \"10:00\"\n",
    "StopA = \"226\"\n",
    "StopB = \"228\"\n",
    "\n",
    "print(prediction_route(PDate, StopA, StopB, PTime))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "weathercall = req.get(f\"https://api.darksky.net/forecast/{config.darksky_api}/53.3498,-6.2603\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather=json.loads(weathercall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'time': 1565858692,\n",
       " 'summary': 'Mostly Cloudy',\n",
       " 'icon': 'partly-cloudy-day',\n",
       " 'nearestStormDistance': 71,\n",
       " 'nearestStormBearing': 31,\n",
       " 'precipIntensity': 0,\n",
       " 'precipProbability': 0,\n",
       " 'temperature': 59.75,\n",
       " 'apparentTemperature': 59.75,\n",
       " 'dewPoint': 50.56,\n",
       " 'humidity': 0.72,\n",
       " 'pressure': 1012.95,\n",
       " 'windSpeed': 18.01,\n",
       " 'windGust': 23.64,\n",
       " 'windBearing': 279,\n",
       " 'cloudCover': 0.51,\n",
       " 'uvIndex': 2,\n",
       " 'visibility': 6.216,\n",
       " 'ozone': 349}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather['currently']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "564.961px",
    "left": "1570.96px",
    "right": "20px",
    "top": "167.969px",
    "width": "756.973px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
