{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Setup and importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T13:42:46.639451Z",
     "start_time": "2019-07-23T13:42:45.962946Z"
    },
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.model_selection import cross_validate\n",
    "from tqdm import tnrange, tqdm_notebook, tqdm\n",
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import scipy.stats as stats\n",
    "import xgboost as xgb\n",
    "import requests as r\n",
    "import pandas as pd\n",
    "import seaborn as s\n",
    "import numpy as np\n",
    "import googlemaps\n",
    "import postgres\n",
    "import holidays\n",
    "import config\n",
    "import json\n",
    "import math\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# parallelisation with Dask for handling large dataframe\n",
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "PB = ProgressBar()\n",
    "PB.register()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T13:42:46.946372Z",
     "start_time": "2019-07-23T13:42:46.939715Z"
    },
    "hidden": true,
    "init_cell": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'config' from '/media/storage/College/S3/Github/DublinBus/Analytics/config.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(postgres.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T13:42:47.262528Z",
     "start_time": "2019-07-23T13:42:47.259779Z"
    },
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bus Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T13:42:51.621496Z",
     "start_time": "2019-07-23T13:42:48.121000Z"
    },
    "init_cell": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d402065dc952>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Drop duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# data = postgres.query(\"SELECT * FROM combined;\", tunnel=True)\n",
    "# data = pd.DataFrame(data)\n",
    "\n",
    "# data = pd.read_csv(\"stored_queries/combined145.csv\")\n",
    "\n",
    "# Dask Version\n",
    "# data = dd.read_csv(\"stored_queries/combined.csv\")\n",
    "\n",
    "# Drop duplicates \n",
    "data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "data.columns = ['dayofservice','tripid','lineid','direction','progrnumber','stopid','plannedDEP','plannedARR','actualDEP','actualARR','routeid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.drop(columns=['routeid','plannedDEP','plannedARR','actualDEP'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T13:42:52.715766Z",
     "start_time": "2019-07-23T13:42:51.774571Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Dask Version\n",
    "# data.dayofservice = dd.to_datetime(data.dayofservice.loc[:])\n",
    "\n",
    "\n",
    "data.dayofservice = pd.to_datetime(data.dayofservice.loc[:])\n",
    "# data.lineid = data.lineid.astype('category')\n",
    "# data.routeid= data.routeid.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T13:42:54.073579Z",
     "start_time": "2019-07-23T13:42:52.773546Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "data.sort_values(by=['dayofservice','lineid','tripid','direction','progrnumber'],inplace=True)\n",
    "# data.to_csv(\"stored_queries/combined145.csv\", index=False, chunksize=500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trips information [for full route prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tripsdata = pd.read_csv(\"stored_queries/trips_df.csv\")\n",
    "tripsdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tripsdata = tripsdata[['dayofservice', 'tripid', 'lineid', 'routeid', 'direction', 'actual_arr', 'actual_dep']]\n",
    "tripsdata.dayofservice = pd.to_datetime(tripsdata.dayofservice)\n",
    "tripsdata.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T13:42:54.176825Z",
     "start_time": "2019-07-23T13:42:54.159974Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "stops = pd.read_csv(\"stop_information.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T13:42:54.244019Z",
     "start_time": "2019-07-23T13:42:54.232737Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "cols = list(stops.columns)\n",
    "cols[0] = 'ix'\n",
    "stops.columns = cols\n",
    "stops.drop(columns=cols[0], inplace=True)\n",
    "\n",
    "stops.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T13:42:54.329499Z",
     "start_time": "2019-07-23T13:42:54.305998Z"
    },
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weather = pd.read_csv(\"stored_queries/weather.csv\")\n",
    "\n",
    "weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T13:42:54.451519Z",
     "start_time": "2019-07-23T13:42:54.443935Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "weather.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T13:42:54.536135Z",
     "start_time": "2019-07-23T13:42:54.529054Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "weather.icon = weather.icon.astype('category')\n",
    "weather.dayofservice = pd.to_datetime(weather.dayofservice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export/ Import the number of stops on each lineid for basic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Read in all lineids from teh database and store in a text file.\n",
    "\n",
    "\n",
    "# lineids = postgres.query(\"Select distinct(lineid) from combined;\", tunnel=True)\n",
    "\n",
    "# q = dict()\n",
    "# for lidx in tnrange(len(lineids)):\n",
    "    \n",
    "#     lid = lineids[lidx]\n",
    "#     q[lid[0]] = postgres.query(\"SELECT MAX(progrnumber) FROM combined WHERE lineid='%s';\" % str(lid[0]), tunnel=True)\n",
    "    \n",
    "# with open(\"stops_per_line.txt\",'w') as f:\n",
    "#     f.write(json.dumps(q))\n",
    "# f.closed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stops_per_line.txt\",'r') as g:\n",
    "    max_stops_per_line = json.loads(g.readlines()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import distances between stops data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "stop_distances = pd.read_csv(\"stored_queries/distancedata.csv\", header=None)\n",
    "stop_distances.columns = ['stopid','previous_stopid','distance']\n",
    "stop_distances.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepairing Data for Combining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weather and leavetimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T13:43:03.479719Z",
     "start_time": "2019-07-23T13:43:02.888810Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# leavetimes data\n",
    "# data.plannedARR = data.dayofservice + pd.to_timedelta(data.plannedARR, unit = 'seconds') # in nanoseconds\n",
    "# data.plannedDEP = data.dayofservice + pd.to_timedelta(data.plannedDEP, unit = 'seconds') # in nanoseconds\n",
    "data.actualARR = data.dayofservice + pd.to_timedelta(data.actualARR, unit = 'seconds') # in nanoseconds\n",
    "# data.actualDEP = data.dayofservice + pd.to_timedelta(data.actualDEP, unit = 'seconds') # in nanoseconds\n",
    "\n",
    "# new columns for combining\n",
    "# data['time_at_stop'] = data.actualDEP - data.actualARR\n",
    "data['weather_merge_time'] = data.actualARR.dt.round('H') #  .dt useful\n",
    "\n",
    "\n",
    "# weather data\n",
    "weather.dayofservice = weather.dayofservice + pd.to_timedelta(weather.hour, unit='hour')\n",
    "\n",
    "# new column for combining\n",
    "weather['rkey'] = weather.dayofservice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Trips data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tripsdata.actual_arr = tripsdata.dayofservice + pd.to_timedelta(tripsdata.actual_arr, unit='seconds')\n",
    "tripsdata.actual_dep = tripsdata.dayofservice + pd.to_timedelta(tripsdata.actual_dep, unit='seconds')\n",
    "tripsdata['triplength'] = tripsdata.actual_arr - tripsdata.actual_dep\n",
    "tripsdata['leavehour'] = tripsdata.actual_dep.dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tripsdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining weather and leavetimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.enable()\n",
    "gc.get_stats()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T13:43:07.234203Z",
     "start_time": "2019-07-23T13:43:06.439577Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "combinedata = data.merge(weather[['icon','temperature','humidity','windSpeed','rain','rkey','hour']], \n",
    "                         left_on='weather_merge_time', \n",
    "                         right_on='rkey', \n",
    "                         how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Free up memory after the data table is deleted. \n",
    "del data\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T13:43:07.894094Z",
     "start_time": "2019-07-23T13:43:07.518939Z"
    },
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# drop lineid as all are 145\n",
    "# combinedata.drop(columns=['rkey','lineid','weather_merge_time','plannedDEP','plannedARR','time_at_stop','actualDEP'], inplace=True)\n",
    "combinedata.drop(columns=['rkey','lineid','weather_merge_time'], inplace=True)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Combining trips and weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tripsdata['weather_merge_time'] = tripsdata.actual_dep.dt.round('H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "combinedtrip = tripsdata.merge(weather[['icon','temperature','humidity','windSpeed','rain','rkey','hour']], \n",
    "                               left_on='weather_merge_time', \n",
    "                               right_on='rkey', \n",
    "                               how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning / Adding Additional features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove inactive stops from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "active_stopids = stops.stopid.values\n",
    "\n",
    "# remove all inactive stops from the dataset. -> additional models that arent needed. \n",
    "combinedata = combinedata[combinedata.stopid.isin(active_stopids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weekday vs weekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T13:43:13.351698Z",
     "start_time": "2019-07-23T13:43:13.185519Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "combinedata['weekend'] = combinedata.dayofservice.dt.weekday.isin([5,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedata.count()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "ie_holidays = holidays.Ireland()\n",
    "combinedata['holiday'] = combinedata.dayofservice.apply(lambda x: x in ie_holidays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair Consecutive Stop IDs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matching progrnumbers to previous stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# previous stopid\n",
    "previousstops =  list(combinedata.stopid)\n",
    "previousstops = np.array(previousstops[:-1]).astype(int)\n",
    "\n",
    "# progrnumber of previous stopid\n",
    "previousstops_progrnumber = list(combinedata.progrnumber)\n",
    "previousstops_progrnumber = np.array(previousstops_progrnumber[:-1]).astype(int)\n",
    "\n",
    "# Actual arrival time of previous stopid\n",
    "previousstops_actualARR = list(combinedata.actualARR)\n",
    "previousstops_actualARR = np.array(previousstops_actualARR[:-1])\n",
    "\n",
    "# Delete the first row of the dataframe to shift the progrnumbers by one. \n",
    "combinedata = combinedata.iloc[1:]\n",
    "\n",
    "# garbage collection to free memory\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combinedata['previous_stopid'] = previousstops\n",
    "combinedata['previous_stopARR'] = previousstops_actualARR\n",
    "combinedata['previous_progrnumber'] = previousstops_progrnumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combinedata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping mis-matched progrnumbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Dropping rows where progrnumber==1 as the first row is currently aligned with the last row of the previous tripid.\n",
    "combinedata = combinedata[combinedata.progrnumber != 1]\n",
    "combinedata.dropna(inplace=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping non-consecutive stop combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# recast type of integer cols from float to int. \n",
    "combinedata.previous_stopid = combinedata.previous_stopid.astype(int)\n",
    "combinedata.previous_progrnumber = combinedata.previous_progrnumber.astype(int)\n",
    "\n",
    "# make progrnumber difference column and then drop anything thats not exactly 1, removes data which skips stops. \n",
    "combinedata['progrnumber_difference'] = combinedata.progrnumber - combinedata.previous_progrnumber\n",
    "\n",
    "# checking how many rows will be left. \n",
    "# combinedata.progrnumber_difference.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# remove non-consecutive stop pairs.\n",
    "combinedata = combinedata[combinedata.progrnumber_difference==1]\n",
    "\n",
    "# Remove additional columns added for this operantion\n",
    "# combinedata.drop(columns=['progrnumber','previous_progrnumber','progrnumber_difference'], inplace=True);\n",
    "\n",
    "# ordering rows [and dropping irrelevant ones: direction, route_id]\n",
    "combinedata = combinedata[['dayofservice', 'tripid','stopid', 'previous_stopid', 'actualARR', 'previous_stopARR',\n",
    "                           'icon', 'temperature', 'humidity', 'windSpeed', 'rain', 'hour', 'weekend', 'holiday']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unique Stopid combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# all unique stop combinations for a given lineid.\n",
    "stop_pairs = combinedata[['stopid','previous_stopid']].drop_duplicates()\n",
    "\n",
    "# print(\"There are %d unique pairs of stops on line: %s\" % (stop_pairs.count()[0], data.lineid.unique()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Travel Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# convert to seconds\n",
    "combinedata['travel_time'] = (combinedata.actualARR - combinedata.previous_stopARR).astype(int)/10**9\n",
    "\n",
    "# drop any values less than 5 seconds [assumed erroneous]\n",
    "combinedata = combinedata[combinedata.travel_time > 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "print(\"There are %d valid pairs\" % combinedata.count()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance between stops [ === Don't run again === ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to get the distance between two stops. \n",
    "# def get_distance(start, finish):\n",
    "#     \"\"\"\n",
    "#     Distance between two (lat,lng) pairs\n",
    "    \n",
    "#     Inputs:\n",
    "#     ================================\n",
    "#     (int) start: stopid of first stop\n",
    "#     (int) finish: stopid of last stop\n",
    "    \n",
    "#     Outputs:\n",
    "#     ===============================\n",
    "#     (int) the distance in metres between the stops. \n",
    "    \n",
    "#     Notes:\n",
    "#     ===============================\n",
    "#     If there is an error, or the api fails to find the distance a value of None will be returned. \n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         begin = (stops[stops.stopid==start ]['lat'].values[0], stops[stops.stopid==start ]['lng'].values[0])\n",
    "#         end   = (stops[stops.stopid==finish]['lat'].values[0], stops[stops.stopid==finish]['lng'].values[0])\n",
    "\n",
    "#     except Exception as e:\n",
    "\n",
    "#         print(start, finish)\n",
    "#         print(repr(e)) \n",
    "#         return None\n",
    "        \n",
    "#     API_key = config.dmatrix_key #enter Google Maps API key\n",
    "#     gmaps = googlemaps.Client(key=API_key)\n",
    "    \n",
    "#     try:\n",
    "#         call = gmaps.distance_matrix(begin, end, mode='walking')\n",
    "    \n",
    "#     except Exception as eL:\n",
    "        \n",
    "#         print(repr(eL))\n",
    "#         return None\n",
    "    \n",
    "#     status = call['status']\n",
    "    \n",
    "#     if status=='OK':\n",
    "#         return call[\"rows\"][0][\"elements\"][0]['distance']['value']\n",
    "    \n",
    "#     else:\n",
    "#         print(status)\n",
    "#         return None\n",
    "\n",
    "# distances_list = []\n",
    "\n",
    "# for index, pair in tqdm_notebook(stop_pairs.iterrows(), total=stop_pairs.shape[0]):\n",
    "\n",
    "# # for pair in stop_pairs.iterrows():\n",
    "# #     start_stopid, finish_stopid = pair[1]\n",
    "\n",
    "#     start_stopid, finish_stopid = pair[0], pair[1]\n",
    "#     distances_list.append(get_distance(start_stopid, finish_stopid))\n",
    "    \n",
    "# distance_array = np.array(distances_list)\n",
    "\n",
    "# stop_pairs['distance'] = distance_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combinedata = combinedata.merge(stop_distances, how='left', left_on=['stopid','previous_stopid'], right_on=['stopid','previous_stopid'])\n",
    "combinedata.distance = combinedata.distance.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Taking average speed as distance / time (km/h)\n",
    "combinedata['avgvel'] = (combinedata.distance / combinedata.travel_time) * (3600/1000)\n",
    "\n",
    "# Note need to drop all data over 120 km/h -> erroneous data\n",
    "combinedata = combinedata[combinedata.avgvel <= 120]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Month/Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "combinedata['month'] = combinedata.dayofservice.dt.month\n",
    "\n",
    "def set_season(x):\n",
    "    winter = [11,12,1]\n",
    "    autumn = [10,9,8]\n",
    "    spring = [4,3,2]\n",
    "\n",
    "    if x in winter:\n",
    "        return 'Winter'\n",
    "    elif x in autumn:\n",
    "        return 'Autumn'\n",
    "    elif x in spring:\n",
    "        return 'Spring'\n",
    "    else:\n",
    "        return 'Summer'\n",
    "    \n",
    "combinedata['season'] = combinedata.dayofservice.dt.month.apply(set_season)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation / boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Note: travel time is continuous but is needed for comparison\n",
    "categorical_features = ['hour','holiday', 'weekend','month','season','icon']\n",
    "\n",
    "continuous_features  = ['travel_time','distance','temperature','windSpeed','rain','humidity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "corr = combinedata[continuous_features].corr()\n",
    "corr.style.background_gradient(cmap='coolwarm').set_precision(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for cat_ft in categorical_features:\n",
    "    combinedata.boxplot(column=['travel_time'], by=cat_ft, grid=False, figsize=(15,6), showfliers=False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Season Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "combinedata.season = combinedata.season.astype('category', categories=['Summer','Spring','Autumn','Winter'])\n",
    "\n",
    "combinedata = pd.concat([combinedata, pd.get_dummies(combinedata.season, prefix='season')], axis=1)\n",
    "combinedata.drop(columns=['season'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Icon Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "combinedata.icon = combinedata.icon.astype('category', categories=['partly-cloudy-day', 'partly-cloudy-night', 'clear-day', 'clear-night', 'rain', 'fog', 'cloudy', 'wind'])\n",
    "\n",
    "combinedata = pd.concat([combinedata, pd.get_dummies(combinedata.icon, prefix='icon')], axis=1)\n",
    "combinedata.drop(columns=['icon'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop all N/A values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "combinedata = combinedata.dropna() # drop na values. \n",
    "combinedata.dtypes\n",
    "\n",
    "print(\"There are %d valid pairs\" % combinedata.count()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "combinedata.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Updating stop pairs\n",
    "# all unique stop combinations for a given lineid.\n",
    "stop_pairs = combinedata[['stopid','previous_stopid']].drop_duplicates()\n",
    "\n",
    "print(\"There are %d unique pairs of stops\" % (stop_pairs.count()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Predictor / Target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "modeldata = combinedata[['travel_time','stopid','previous_stopid','distance',\n",
    "                         'temperature','humidity', 'windSpeed', 'rain', 'hour', 'holiday', 'weekend',\n",
    "                         'month','season_Winter','season_Autumn','season_Summer','season_Spring',\n",
    "                         'icon_clear-day', 'icon_clear-night', 'icon_cloudy', 'icon_fog',\n",
    "                         'icon_partly-cloudy-day', 'icon_partly-cloudy-night', 'icon_rain','icon_wind']]\n",
    "modeldata.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# need to put this in a loop over the pairs of stops. (unique)\n",
    "target     = ['travel_time']\n",
    "predictors = ['temperature','humidity', 'windSpeed', 'rain', 'hour', 'holiday', 'weekend',\n",
    "              'month','season_Winter','season_Autumn','season_Summer','season_Spring',\n",
    "              'icon_clear-day', 'icon_clear-night', 'icon_cloudy', 'icon_fog',\n",
    "              'icon_partly-cloudy-day', 'icon_partly-cloudy-night', 'icon_rain','icon_wind']\n",
    "\n",
    "General_predictors = ['temperature','humidity', 'windSpeed', 'rain', 'hour', \n",
    "                      'holiday', 'weekend','month','distance',\n",
    "                      'season_Winter','season_Autumn','season_Summer','season_Spring',\n",
    "                      'icon_clear-day', 'icon_clear-night', 'icon_cloudy', 'icon_fog',\n",
    "                      'icon_partly-cloudy-day', 'icon_partly-cloudy-night', \n",
    "                      'icon_rain','icon_wind']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "print(\"There are %d rows in model data\" % (modeldata.count()[0]))\n",
    "print(\"Average Travel Time between stops: {}\".format(modeldata.travel_time.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Model for stops not in the data [ will take ages to train ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "travel_time_deviation = modeldata.travel_time.std()\n",
    "\n",
    "# Only allow travel times greater than zero \n",
    "General_modeldata = modeldata[modeldata.travel_time >= 0]\n",
    "\n",
    "# Filter outliers from the dataset\n",
    "General_modeldata = General_modeldata[abs(General_modeldata.travel_time-General_modeldata.travel_time.mean()) < 3*travel_time_deviation]\n",
    "\n",
    "# Show spread of data\n",
    "General_modeldata.travel_time.hist(bins=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# ========================= Test/Train Splits ========================== #\n",
    "General_X_train, General_X_test, General_y_train, General_y_test = train_test_split(General_modeldata[General_predictors],\n",
    "                                                                                     General_modeldata[target].values.ravel(), \n",
    "                                                                                     test_size=0.2, \n",
    "                                                                                     shuffle=True)\n",
    "General_X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tTest-rmse:77.3825\n",
      "Will train until Test-rmse hasn't improved in 100 rounds.\n",
      "[50]\tTest-rmse:32.4332\n",
      "[100]\tTest-rmse:31.119\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-ff1a0be7815d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mnum_rounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mGeneral_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeneral_dtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_rounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGeneral_dtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# ============================  Predictions ============================ #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/BusEnv/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/BusEnv/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/BusEnv/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1110\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ========================== Making DMatrices ========================== #\n",
    "General_dtrain = xgb.DMatrix(General_X_train, label=General_y_train)\n",
    "General_dtest = xgb.DMatrix(General_X_test, label=General_y_test)\n",
    "\n",
    "# =========================== Training Model =========================== #\n",
    "\n",
    "param = {\n",
    "    'eta':0.1,\n",
    "    'max_depth':6\n",
    "}\n",
    "num_rounds = 10000\n",
    "\n",
    "General_model = xgb.train(param, General_dtrain, num_rounds, evals=[(General_dtest, 'Test')], verbose_eval=50, early_stopping_rounds=100)\n",
    "\n",
    "# ============================  Predictions ============================ #\n",
    "General_xgbpreds = General_model.predict(General_dtest)\n",
    "\n",
    "# ====================== Feature Importance graph ====================== #\n",
    "# xgb.plot_importance(General_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Wise Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training Model for all pairs of stops in the dataset\n",
    "\n",
    "models = dict()\n",
    "emptys = []\n",
    "metric = dict()\n",
    "\n",
    "no_stops = stop_pairs.count()[0]\n",
    "print(\"There are %d models to train.\" % no_stops)\n",
    "\n",
    "for pair in tqdm_notebook(stop_pairs.iterrows(), desc=\"Progress: \", total=no_stops):\n",
    "\n",
    "    # Travelling From A -> B\n",
    "    A = pair[1][1]\n",
    "    B = pair[1][0]\n",
    "    \n",
    "    # ========================= Rows from A -> B =========================== #\n",
    "    data = modeldata[(modeldata.stopid==B)&(modeldata.previous_stopid==A)]\n",
    "    \n",
    "    if data.count()[0] > 10:\n",
    "    \n",
    "        # ========================= Removing Outliers ========================== #\n",
    "        travel_sigma = data.travel_time.std()\n",
    "\n",
    "        # Only allow travel times greater than zero \n",
    "        data = data[data.travel_time >= 0]\n",
    "\n",
    "        # Filter outliers from the dataset [ 2σ as the cutoff ~95% of data ]\n",
    "        data = data[abs(data.travel_time - data.travel_time.mean()) < 2*travel_sigma]\n",
    "\n",
    "        # ========================= Remove Null Data =========================== #\n",
    "        data.dropna(inplace=True)\n",
    "\n",
    "        # ========================= Test/Train Splits ========================== #\n",
    "        X_train, X_test, y_train, y_test = train_test_split(data[predictors],data[target].values.ravel(), test_size=0.3, shuffle=True)\n",
    "\n",
    "        # ========================== Making DMatrices ========================== #\n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "        # =========================== Training Model =========================== #\n",
    "\n",
    "        param = {\n",
    "            'eta': 0.15,\n",
    "            'max_depth': 6\n",
    "        }\n",
    "\n",
    "        num_rounds = 10000\n",
    "\n",
    "        try:\n",
    "            \n",
    "            # ============ Train ============= #\n",
    "            model = xgb.train(param, dtrain, num_rounds, evals=[(dtest, 'Test')], verbose_eval=False, early_stopping_rounds=100)\n",
    "            models[f'{A}_{B}'] = model\n",
    "            \n",
    "            # ============ Testing Accuracy ========== #\n",
    "            preds = model.predict(dtest)\n",
    "            metric[f'{A}_{B}'] = dict()\n",
    "            metric[f'{A}_{B}']['rmse'] = np.sqrt(metrics.mean_squared_error(preds, y_test))\n",
    "            metric[f'{A}_{B}']['preds']= preds\n",
    "            metric[f'{A}_{B}']['ytest']= y_test\n",
    "            \n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with route: {A} -> {B}\")\n",
    "            data.head()\n",
    "            y_train, y_test\n",
    "            print(repr(e), end='\\n================================================\\n')\n",
    "    else:\n",
    "        print(f\"Empty Set Error: {A} -> {B}\")\n",
    "        emptys.append((A,B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================= Visualising Errors ========================= #\n",
    "General_xgbresiduals = General_xgbpreds - General_y_test\n",
    "\n",
    "# best fit of data\n",
    "(xmu, xsigma) = stats.norm.fit(General_xgbresiduals)\n",
    "\n",
    "# # The histogram of the data\n",
    "xn, xbins, xpatches = plt.hist(General_xgbresiduals, 100, density=True, facecolor='blue', alpha=0.6)\n",
    "\n",
    "# add a 'best fit' line\n",
    "xy = mlab.normpdf(xbins, xmu, xsigma)\n",
    "xl = plt.plot(xbins, xy, 'r--', linewidth=2)\n",
    "plt.show()\n",
    "\n",
    "print(xsigma,xmu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "General_rmse = np.sqrt(metrics.mean_squared_error(General_xgbpreds, General_y_test))\n",
    "\n",
    "print(f\"\"\"\n",
    "General Model RMSE: {General_rmse}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores_sample = []\n",
    "for key in metric.keys():\n",
    "        \n",
    "    scores_sample.append(metric[key]['rmse'])\n",
    "    \n",
    "    if metric[key]['rmse'] > 50:\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.title(\"{} - {}\".format(key, metric[key]['rmse']))\n",
    "        plt.plot(metric[key]['preds'],'r')\n",
    "        plt.plot(metric[key]['ytest'],'b', alpha=0.5)\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(scores_sample, 'b.')\n",
    "plt.axhline(y=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting General Model\n",
    "\n",
    "General_model.save_model(\"ModelFiles/StopModels/General.model\")\n",
    "\n",
    "\n",
    "# Exporting all models \n",
    "    \n",
    "mkeys = list(models.keys())\n",
    "\n",
    "for k in tnrange(len(mkeys)):\n",
    "    \n",
    "    key = mkeys[k]\n",
    "    \n",
    "    models[key].save_model(f\"ModelFiles/StopModels/{key}.model\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "notify_time": "10",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "463.083px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "672.958px",
    "left": "1997px",
    "right": "20px",
    "top": "65px",
    "width": "506.958px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
